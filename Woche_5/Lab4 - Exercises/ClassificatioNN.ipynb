{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying movie reviews\n",
    "\n",
    "In this example, we will learn to classify movie reviews into \"positive\" reviews and \"negative\" reviews, just based on the text content of the reviews. This is a \"binary classification example\".\n",
    "\n",
    "## The Dataset\n",
    "\n",
    "The IMDB dataset: a set of 50,000 reviews from the Internet Movie Database. They are split into 25,000 reviews for training and 25,000 reviews for testing, each set consisting in 50% negative and 50% positive reviews.\n",
    "\n",
    "The IMDB dataset comes pre-built in Keras. It has already been preprocessed: the reviews (sequences of words) have been turned into sequences of integers, where each integer stands for a specific word in a dictionary.\n",
    "\n",
    "The following code will load the dataset (when you run it for the first time, about 80MB of data will be downloaded to your machine):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Check training and test data dimension. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the Data\n",
    "\n",
    "We cannot feed lists of integers into a neural network. We have to turn lists into tensors.\n",
    "\n",
    "We could pad our lists so that they all have the same length, and turn them into an integer tensor of shape (samples, word_indices), then use as first layer in our network a layer capable of handling such integer tensors (the Embedding layer, which we will cover in detail later).\n",
    "We could one-hot-encode our lists to turn them into vectors of 0s and 1s. Concretely, this would mean for instance turning the sequence [3, 5] into a 10,000-dimensional vector that would be all-zeros except for indices 3 and 5, which would be ones. Then we could use as first layer in our network a Dense layer, capable of handling floating point vector data.\n",
    "We will go with the latter solution. Let's vectorize our data, which we will do manually for maximum clarity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    # Create an all-zero matrix of shape (len(sequences), dimension)\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.  # set specific indices of results[i] to 1s\n",
    "    return results\n",
    "\n",
    "# Our vectorized training data\n",
    "x_train = vectorize_sequences(train_data)\n",
    "# Our vectorized test data\n",
    "x_test = vectorize_sequences(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Check one sample. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our vectorized labels\n",
    "y_train = np.asarray(train_labels).astype('float32')\n",
    "y_test = np.asarray(test_labels).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input data is simply vectors, and the labels are scalars (1 and 0): this is the easiest setup you will ever encounter. \n",
    "Key questions: how many layers do we need? How many hidden units for each layer? \n",
    "We will build a simple stack of fully-connected (Dense) layers. The network should have the following structure:\n",
    "- Dense layer with 16 hidden units and ReLU activation function;\n",
    "- Dense layer with 16 hidden units and ReLU activation function;\n",
    "- Dense layer output with sigmoid activation function;\n",
    "\n",
    "Remember: for the fully connected layer, it holds: output = relu(dot(W, input) + b), with W weight matrix and b bias.\n",
    "Remember: the sigmoid function will give you a score between 0 and 1, which tells you \"how likely the sample is to have \"1\", that means the review to be \"positive\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import models and layers from Keras. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create the network model as described above. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Configure the model with an  optimizer and  a loss function. Check Keras documentation and try to understand which one suits better for your problem. Use `accuracy` as metrics.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Do the same as in the previous cell but pass an optimizer class instance as the optimizer argument. Check Keras documentation for more insights. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Do the same as in the previous cell, but pass function objects as the loss or metrics arguments. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validating the approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create the Validation set of 10,000 samples and the Training one, consequentily. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Train the model for 20 epochs in mini-batches of 512 samples. Use the Validation set you have just created. Check Keras documentation for more info. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Check the `.history` of the result of your fit and print it to see in which format it is and how it is possible to access to the values. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Complete the #TO DO to obtain the plot of the training and validation accuracy. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = #TO DO: take from the history dictionary of the model the training accuracy\n",
    "val_acc =  #TO DO: take from the history dictionary of the model the validation accuracy\n",
    "loss =  #TO DO: take from the history dictionary of the model the training loss\n",
    "val_loss =  #TO DO: take from the history dictionary of the model the validation loss \n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "# Plot the Loss\n",
    "\n",
    "# \"bo\" is for \"blue dot\"\n",
    "plt.plot(epochs,#TO DO: use the training loss, 'bo', label='Training loss') \n",
    "# b is for \"solid blue line\"\n",
    "plt.plot(epochs,  #TO DO: use the validation loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Accuracy\n",
    "\n",
    "plt.clf()   # clear figure\n",
    "acc_values =  #TO DO: take from the history dictionary of the model the training accuracy\n",
    "val_acc_values =  # TO DO: take from the history dictionary of the model the validation accuracy\n",
    "\n",
    "# Plot Epochs vs Training accuracy and Epochs vs Validation accuracy\n",
    "plt.plot(epochs, #TO DO: use training accuracy,  'bo', label='Training acc')  \n",
    "plt.plot(epochs, #TO DO: use validation accuracy,, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** What can you conclude from these results? Try to analyse and understand them. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train a new network from scratch for four epochs, then evaluate it on our test data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Train a new network from scratch for 4 epochs and evaluate it on the test data. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Make predictions with your model. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further experiments\n",
    "**NOTE: In the next Lab, we will focus more in optimizing the hyperparameters and understanding how they may influence the outcomes. However here are some simple experiments you can also try now.**\n",
    "\n",
    "** Try out these experiments and every time re-compute the accuracy to check how it changes. **\n",
    "#### Experiment 1\n",
    "We were using 2 hidden layers, try to use 1 or 3 hidden layers and see how it affects validation and test accuracy.\n",
    "#### Experiment 2\n",
    "Try to use layers with more hidden units or less hidden units: 32 units, 64 units...\n",
    "#### Experiment 3\n",
    "Try to use the another loss function.\n",
    "#### Experiment 4\n",
    "Try to use the tanh activation instead of ReLU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

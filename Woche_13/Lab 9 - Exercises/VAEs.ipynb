{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Autoencoders\n",
    "\n",
    "In technical terms, here is how a variational autoencoder works. First, an encoder module turns the input samples input_img into two parameters in a latent space of representations, which we will note z_mean and z_log_variance. Then, we randomly sample a point z from the latent normal distribution that is assumed to generate the input image, via z = z_mean + exp(z_log_variance) * epsilon, where epsilon is a random tensor of small values. Finally, a decoder module will map this point in the latent space back to the original input image. Because epsilon is random, the process ensures that every point that is close to the latent location where we encoded input_img (z-mean) can be decoded to something similar to input_img, thus forcing the latent space to be continuously meaningful. Any two close points in the latent space will decode to highly similar images. Continuity, combined with the low dimensionality of the latent space, forces every direction in the latent space to encode a meaningful axis of variation of the data, making the latent space very structured and thus highly suitable to manipulation via concept vectors.\n",
    "\n",
    "The parameters of a VAE are trained via two loss functions: first, a reconstruction loss that forces the decoded samples to match the initial inputs, and a regularization loss, which helps in learning well-formed latent spaces and reducing overfitting to the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go over a Keras implementation of a VAE. Schematically, it looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the input into a mean and variance parameter\n",
    "z_mean, z_log_variance = encoder(input_img)\n",
    "\n",
    "# Draw a latent point using a small random epsilon\n",
    "z = z_mean + exp(z_log_variance) * epsilon\n",
    "\n",
    "# Then decode z back to an image\n",
    "reconstructed_img = decoder(z)\n",
    "\n",
    "# Instantiate a model\n",
    "model = Model(input_img, reconstructed_img)\n",
    "\n",
    "# Then train the model using 2 losses:\n",
    "# a reconstruction loss and a regularization loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Build the encoder network in this way (it is a simple convnet which maps the input image x to two vectors, z_mean and z_log_variance). **\n",
    "\n",
    "- Convolution layer of size 32, with 3x3 kernel and ReLU activation function;\n",
    "- Convolution layer of size 64, with 3x3 kernel, stride 2x2 and ReLU activation function;\n",
    "- Convolution layer of size 64, with 3x3 kernel, and ReLU activation function;\n",
    "- Convolution layer of size 64, with 3x3 kernel, and ReLU activation function;\n",
    "- Flatten layer;\n",
    "- Fully Connected layer of size 32 with ReLU activation function;\n",
    "- Fully Connected layer of size `latent_dim` (assign the result of this to a variable called `z_mean`)\n",
    "- Fully Connected layer of size `latent_dim` (assign the result of this to a variable called `z_log_var`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import layers\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "import numpy as np\n",
    "\n",
    "img_shape = (28, 28, 1)\n",
    "batch_size = 16\n",
    "latent_dim = 2  # Dimensionality of the latent space: a plane\n",
    "\n",
    "input_img = keras.Input(shape=img_shape)\n",
    "\n",
    "### TO DO: build the encoder here. ###\n",
    "\n",
    "z_mean = layers.Dense(latent_dim)(x)\n",
    "z_log_var = layers.Dense(latent_dim)(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the code for using z_mean and z_log_var, the parameters of the statistical distribution assumed to have produced input_img, to generate a latent space point z."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    epsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim),\n",
    "                              mean=0., stddev=1.)\n",
    "    return z_mean + K.exp(z_log_var) * epsilon\n",
    "\n",
    "z = layers.Lambda(sampling)([z_mean, z_log_var])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Build the decoder network in this way: **\n",
    "- Upsample to the correct number of units\n",
    "- Reshape into an image of the same shape as before our last `Flatten` layer\n",
    "- Apply the reverse operation to the initial stack of convolution layers: a 'Conv2DTranspose' layer with corresponding parameters\n",
    "- Convolutional layer, 1 filter of dimension3x3 with `sigmoid` activation function\n",
    "\n",
    "We end up with a feature map of the same size as of the original input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the decoder implementation: we reshape the vector z to the dimensions of an image, then we use a few convolution layers to obtain a final image output that has the same dimensions as the original input_img."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the input where we will feed `z`.\n",
    "decoder_input = layers.Input(K.int_shape(z)[1:])\n",
    "\n",
    "\n",
    "### TO DO: build the decoder here ####\n",
    "\n",
    "\n",
    "# This is our decoder model.\n",
    "decoder = Model(decoder_input, x)\n",
    "\n",
    "# We then apply it to `z` to recover the decoded `z`.\n",
    "z_decoded = decoder(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dual loss of a VAE doesn't fit the traditional expectation of a sample-wise function of the form loss(input, target). Thus, we set up the loss by writing a custom layer with internally leverages the built-in add_loss layer method to create an arbitrary loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomVariationalLayer(keras.layers.Layer):\n",
    "\n",
    "    def vae_loss(self, x, z_decoded):\n",
    "        x = K.flatten(x)\n",
    "        z_decoded = K.flatten(z_decoded)\n",
    "        xent_loss = keras.metrics.binary_crossentropy(x, z_decoded)\n",
    "        kl_loss = -5e-4 * K.mean(\n",
    "            1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "        return K.mean(xent_loss + kl_loss)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = inputs[0]\n",
    "        z_decoded = inputs[1]\n",
    "        loss = self.vae_loss(x, z_decoded)\n",
    "        self.add_loss(loss, inputs=inputs)\n",
    "        # We don't use this output.\n",
    "        return x\n",
    "\n",
    "# We call our custom layer on the input and the decoded output,\n",
    "# to obtain the final model output.\n",
    "y = CustomVariationalLayer()([input_img, z_decoded])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we instantiate and train the model. Since the loss has been taken care of in our custom layer, we don't specify an external loss at compile time (loss=None), which in turns means that we won't pass target data during training (as you can see we only pass x_train to the model in fit)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Create a VAE model and pass `input_img` and `y` (the output) to it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Import the MNIST dataset from Keras. Remember that we do not use the training labels! Perform some data pre-processing: use `astype('float32')` and then `reshape`. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Fit the VAE model with 10 epochs and test data as validation data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once such a model is trained -- e.g. on MNIST, in our case -- we can use the decoder network to turn arbitrary latent space vectors into images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Display a 2D manifold of the digits\n",
    "n = 15  # figure with 15x15 digits\n",
    "digit_size = 28\n",
    "figure = np.zeros((digit_size * n, digit_size * n))\n",
    "# Linearly spaced coordinates on the unit square were transformed\n",
    "# through the inverse CDF (ppf) of the Gaussian\n",
    "# to produce values of the latent variables z,\n",
    "# since the prior of the latent space is Gaussian\n",
    "grid_x = norm.ppf(np.linspace(0.05, 0.95, n))\n",
    "grid_y = norm.ppf(np.linspace(0.05, 0.95, n))\n",
    "\n",
    "for i, yi in enumerate(grid_x):\n",
    "    for j, xi in enumerate(grid_y):\n",
    "        z_sample = np.array([[xi, yi]])\n",
    "        z_sample = np.tile(z_sample, batch_size).reshape(batch_size, 2)\n",
    "        x_decoded = decoder.predict(z_sample, batch_size=batch_size)\n",
    "        digit = x_decoded[0].reshape(digit_size, digit_size)\n",
    "        figure[i * digit_size: (i + 1) * digit_size,\n",
    "               j * digit_size: (j + 1) * digit_size] = digit\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(figure, cmap='Greys_r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The grid of sampled digits shows a completely continuous distribution of the different digit classes, with one digit morphing into another as you follow a path through latent space. Specific directions in this space have a meaning, e.g. there is a direction for \"four-ness\", \"one-ness\", etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
